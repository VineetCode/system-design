# âœ… What is Rate Limiting?
It is controlling how many requests a client (user, IP, token) can make in a given time window.

ğŸ‘‰ Helps protect backend resources from abuse or overload.

# âœ… Why is Rate Limiting needed?
Prevent API abuse (ex: bot making 1000s of requests)

Ensure fair usage (all users get fair share of resources)

Protect DB / Cache / Service from being overwhelmed

Prevent DoS attacks

Save cost (APIs like OpenAI, AWS, etc)

Manage 3rd-party quotas (if youâ€™re calling 3rd-party APIs)

# âœ… What is a typical rule?
Per IP â†’ "Allow 5 requests per minute per IP"

Per User â†’ "Allow 1000 requests/day per user"

Per API Key â†’ "Allow 500 requests/hour per API key"

# âœ… Where to put Rate Limiting?
| Level             | Example                              |
| ----------------- | ------------------------------------ |
| Client-side       | Mobile app limits user button clicks |
| CDN/Edge          | Cloudflare / Akamai edge rate limits |
| API Gateway       | AWS API Gateway / Kong / NGINX       |
| Application Layer | Code-level rate limit                |
| Database Layer    | Rare (usually not here)              |

Best â†’ API Gateway or App layer.

# âœ… Algorithms used
| Algorithm                | Description                         |
| ------------------------ | ----------------------------------- |
| Fixed Window             | Count requests per window (per min) |
| Sliding Window           | Smooth version of fixed window      |
| Token Bucket             | Allow bursts, refill over time      |
| Leaky Bucket             | Enforce constant flow               |
| Distributed Token Bucket | Rate limit across multiple servers  |

# âœ… Example in System Design Question
ğŸ‘‰ Suppose interviewer asks:
"Design an API for uploading images. Millions of users. How will you rate-limit?"

## You should answer:
### 1ï¸âƒ£ Where to Rate Limit?

CDN level â†’ basic IP protection

API Gateway level â†’ rate limit per API key/user

Application layer â†’ for finer control (VIP users)


### #ï¸âƒ£ How to implement?

In-memory for small services â†’ simple Map (not distributed)

Redis-based for distributed rate limiting

All servers check Redis for counts

Redis TTL for window expiration

Atomic INCR / DECR operations

### 3ï¸âƒ£ Algorithm choice?
| Algorithm      | When to use                                           |
| -------------- | ----------------------------------------------------- |
| Fixed Window   | Very simple cases                                     |
| Sliding Window | Smooth limits                                         |
| Token Bucket   | **Most flexible** (allow bursts, used in Stripe, AWS) |
| Leaky Bucket   | **Steady flow** (traffic shaping)                     |

Production API â†’ Token Bucket with Redis backend

## âœ… Redis-based Rate Limiter
Each IP/user â†’ key in Redis
Example: rate_limit:user123:minute

Redis TTL = window size

Use atomic INCR â€” if > limit â†’ reject (429)

## âœ… Real-world Example
Stripe API Rate Limit:

100 requests per second â†’ per API key

If exceeded â†’ 429 Too Many Requests

Retry-After header is returned

## âœ… How to answer in interview?
When interviewer asks "How will you Rate Limit?", say:

ğŸ—£ï¸
ğŸ‘‰ Iâ€™ll use Token Bucket algorithm â†’ allows bursts, refill tokens over time
ğŸ‘‰ Use Redis-based distributed limiter â†’ supports horizontal scaling
ğŸ‘‰ Each server checks Redis before serving request
ğŸ‘‰ For VIP users â†’ custom limits
ğŸ‘‰ Use Retry-After header to help clients know when to retry

## âœ… Example Redis Lua Script (advanced)

local current
current = redis.call("INCR", KEYS[1])
if tonumber(current) == 1 then
   redis.call("EXPIRE", KEYS[1], ARGV[1])
end
if tonumber(current) > tonumber(ARGV[2]) then
  return 0
else 
  return 1
end

## âœ… Summary
When interviewer asks about Rate Limiting:

âœ… Mention use-cases (abuse, fairness, DoS)
âœ… Mention where (CDN, API Gateway, App layer)
âœ… Mention algorithms (Token Bucket, Sliding Window)
âœ… For distributed â†’ use Redis
âœ… Mention VIP users, Retry-After header


Here is a full ready-to-use System Design interview answer to:
# "How would you design a Rate Limiter?"

## 1ï¸âƒ£ What is Rate Limiting?
ğŸ‘‰ Rate limiting controls the number of requests a client (IP, user, API key) can send to an API in a defined time window.
It protects services from abuse, ensures fairness, avoids system overload, and prevents Denial-of-Service (DoS) attacks.

## 2ï¸âƒ£ Where to apply Rate Limiting?
CDN Level â†’ e.g., Cloudflare â†’ stop attacks at edge

API Gateway Level â†’ e.g., AWS API Gateway / Kong

Application Layer â†’ fine-grained control

Per Endpoint â†’ e.g., login endpoint stricter

ğŸ‘‰ In my design, I would implement it at both API Gateway and Application Layer.

## 3ï¸âƒ£ Key Requirements
âœ… Support multiple users (millions of users)
âœ… Support different limits per user (normal vs VIP)
âœ… Should work in distributed system
âœ… Low latency â€” should not slow API
âœ… Resilient if cache/db is down
âœ… Show proper 429 response with Retry-After header

## 4ï¸âƒ£ Algorithm to use?
I would choose Token Bucket â€” because:
| Algorithm      | Why?                                                                |
| -------------- | ------------------------------------------------------------------- |
| Token Bucket   | Allows bursts, refill smoothly, most flexible (used by Stripe, AWS) |
| Leaky Bucket   | Enforces steady rate â€” smoothing                                    |
| Fixed Window   | Simple but bursty (not accurate)                                    |
| Sliding Window | Smoother than fixed, but more memory                                |

## 5ï¸âƒ£ Storage?
For distributed rate limiting, we need shared state across servers â†’ Redis is ideal.
| Storage Option | Why?                            |
| -------------- | ------------------------------- |
| In-memory      | Simple, but not distributed     |
| Redis          | Fast, atomic ops, supports TTL  |
| DB (SQL/NoSQL) | Too slow for per-request checks |

ğŸ‘‰ So Iâ€™d use Redis, with keys like:
rate_limit:{user_id} or rate_limit:{ip}

## 6ï¸âƒ£ How to implement?
Token Bucket Logic:

Each client (IP/user) has a token bucket:

Capacity = max tokens (limit)

Refill rate = tokens/sec

When a request comes in:

If bucket has token â†’ consume token â†’ allow

Else â†’ return 429 Too Many Requests

How tokens refill?
On each request, compute:

tokens_to_add = (current_time - last_updated_time) * refill_rate

## 7ï¸âƒ£ How to make atomic in Redis?

Use Lua script in Redis:

INCR token count

EXPIRE key

All atomic â†’ avoid race condition

Example Redis Lua:

local tokens = redis.call('GET', KEYS[1])
if tokens == false then
tokens = ARGV[1]
end
if tonumber(tokens) > 0 then
redis.call('DECR', KEYS[1])
return 1
else
return 0
end

## 8ï¸âƒ£ What response?
On 429, return:

HTTP 429 Too Many Requests

Retry-After header â†’ tells client when to retry

## 9ï¸âƒ£ Example limits

# 10ï¸âƒ£ Scaling
Redis cluster for horizontal scaling

Can shard Redis by user ID hash

Use connection pool to Redis

## Final Summary:
ğŸ‘‰ I would design the Rate Limiter using Token Bucket algorithm, stored in Redis,
ğŸ‘‰ Implemented in both API Gateway and Application Layer,
ğŸ‘‰ Using atomic Lua script,
ğŸ‘‰ With support for per-user configs, Retry-After header,
ğŸ‘‰ Scalable to millions of users.

# âœ…Deque Rate Limit

## How It Works:
## âœ… We use a ConcurrentHashMap<String, Deque<Long>>
## âœ… For each IP, store timestamps of recent requests
## âœ… On each request:

Clean up timestamps older than 1 min

If requests in window < 5 â†’ allow and record timestamp

If â‰¥ 5 â†’ return 429

## Output:

200 OK - Request allowed for IP: 192.168.1.10

200 OK - Request allowed for IP: 192.168.1.10

200 OK - Request allowed for IP: 192.168.1.10

200 OK - Request allowed for IP: 192.168.1.10

200 OK - Request allowed for IP: 192.168.1.10

429 Too Many Requests for IP: 192.168.1.10

429 Too Many Requests for IP: 192.168.1.10

# ğŸš€ Why Sliding Window?
The naive approach stores all timestamps â€” memory usage grows if lots of IPs send many requests.

Sliding Window Counter keeps just a counter + "last reset time" per IP â€” it's lighter and faster.

## âœ… Key Points:
For each IP â†’ store a Counter object:

count â†’ how many requests in current window

windowStart â†’ when the window started

If current time > windowStart + window size â†’ reset counter

Memory per IP = O(1) â†’ just 2 variables per IP

## ğŸš€ Advantages over the Deque-based version:
âœ… Lower memory â€” just 1 object per IP
âœ… No need to store or iterate timestamps
âœ… Very fast O(1) checks
âœ… Thread-safe with simple sync

# âœ… 1. Token Bucket Algorithm
How it works:

Each IP has a "bucket" with max N tokens (say 5).

Each request removes 1 token.

Tokens refill over time (e.g., 1 token per 12 seconds â†’ 5 tokens per min).

If bucket empty â†’ reject request.

ğŸ‘‰ Smooth rate limiting (better for real-time APIs).

## âœ… Pros:

Smooth refill

Burst handling (if IP sends no requests for a while, tokens build up again)

Used in Stripe, AWS API Gateway

# âœ… 2. Leaky Bucket Concept:
Bucket with capacity (say, 5 "drops")

Drops leak at constant rate (ex: 1 drop per 12 sec)

Requests try to "fill" bucket:

If bucket not full â†’ allow request â†’ add drop

If bucket full â†’ reject (429)

Leaks continue over time â€” bucket refills automatically

ğŸ‘‰ Forces a steady flow â†’ good for smoothing bursty traffic.

## âœ… What happens here?
Bucket size: 5

Leak rate: 1 token per 12 sec

Each request = adding 1 token

If bucket full â†’ 429

If tokens leak out â†’ new space opens up â†’ allow request

# âœ… 3. Google Guava RateLimiter
If you want a production-quality ready implementation, use Guava RateLimiter:

## âœ… Pros:

Production ready

High-performance

Well-tested

You can set rate per second, minute, etc

Supports warm-up period if needed

| Algorithm         | Use case                         |
| ----------------- | -------------------------------- |
| Sliding Window    | Simple, fast, less memory        |
| Token Bucket      | Smooth refill, handles bursts    |
| Leaky Bucket      | Traffic shaping (steady outflow) |
| Guava RateLimiter | Production-ready, flexible       |


##  Here you go â€” a Spring Boot-ready example of using an Interceptor + in-memory RateLimiter (Sliding Window or Token Bucket) ğŸ‘‡

## âœ… Structure
Interceptor â€” runs before every request

For each IP â†’ check if allowed â†’ allow or return 429

No 3rd party libs needed (Guava optional)

Works with any controller

âœ… Result
First 5 requests in 1 min â†’ 200 OK

Next requests â†’ 429 Too Many Requests

Auto resets after window

âœ… Summary
Lightweight (no 3rd party required)

No annotations needed â€” works at middleware level

Can extend to:

Use TokenBucketRateLimiter instead

Add Redis cache to make it distributed (if running multiple servers)

Use Guava RateLimiter per IP

